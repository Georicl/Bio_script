#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pickle
import csv
import argparse


def convert_lefse_to_csv(lefse_in_path, sample_map_path, output_csv_path):
    """
    å°† LEfSe çš„ .in è¾“å…¥æ–‡ä»¶å’Œæ ·æœ¬æ˜ å°„æ–‡ä»¶è½¬æ¢ä¸ºæ ‡å‡†çš„ CSV è¡¨æ ¼ã€‚

    Args:
        lefse_in_path (str): LEfSe .in æ–‡ä»¶çš„è·¯å¾„ (pickle æ ¼å¼)ã€‚
        sample_map_path (str): æ ·æœ¬æ˜ å°„æ–‡ä»¶çš„è·¯å¾„ (ç¬¬ä¸€åˆ—æ ·æœ¬IDï¼Œç¬¬äºŒåˆ—åˆ†ç»„ID)ã€‚
        output_csv_path (str): è¾“å‡º CSV æ–‡ä»¶çš„è·¯å¾„ã€‚
    """
    print("ğŸš€ å¼€å§‹è½¬æ¢ LEfSe æ–‡ä»¶...")

    # --- 1. è¯»å–æ ·æœ¬å’Œåˆ†ç»„ä¿¡æ¯ ---
    # è¿™ä¸ªé¡ºåºå†³å®šäº†æ•°æ®åˆ—çš„é¡ºåº
    sample_ids = []
    group_ids = []
    try:
        with open(sample_map_path, 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    sample_ids.append(parts[0])
                    group_ids.append(parts[1])
        print(f"âœ… æˆåŠŸè¯»å– {len(sample_ids)} ä¸ªæ ·æœ¬ä¿¡æ¯ã€‚")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ ·æœ¬æ˜ å°„æ–‡ä»¶ '{sample_map_path}'")
        return

    # --- 2. åŠ è½½ lefse.in æ–‡ä»¶ ---
    try:
        with open(lefse_in_path, 'rb') as f:
            # ä½¿ç”¨ pickle.load() æ¥è§£æäºŒè¿›åˆ¶æ–‡ä»¶
            lefse_data = pickle.load(f)
        print("âœ… æˆåŠŸåŠ è½½ lefse.in æ–‡ä»¶ã€‚")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° LEfSe è¾“å…¥æ–‡ä»¶ '{lefse_in_path}'")
        return
    except pickle.UnpicklingError:
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ '{lefse_in_path}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ pickle æ–‡ä»¶ã€‚")
        return

    # ä»åŠ è½½çš„æ•°æ®ä¸­æå–ç‰©ç§ä¸°åº¦ä¿¡æ¯
    features_data = lefse_data.get('feats', {})
    if not features_data:
        print("âŒ é”™è¯¯: lefse.in æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'feats' æ•°æ®ã€‚")
        return

    # --- 3. å†™å…¥ CSV æ–‡ä»¶ ---
    with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)

        # å†™å…¥ç¬¬ä¸€è¡Œè¡¨å¤´ï¼šåˆ†ç»„ä¿¡æ¯
        # ç¬¬ä¸€ä¸ªå•å…ƒæ ¼ç•™ç©ºæˆ–å†™ 'Group'ï¼Œåé¢æ˜¯æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„åˆ†ç»„
        writer.writerow(['Group'] + group_ids)

        # å†™å…¥ç¬¬äºŒè¡Œè¡¨å¤´ï¼šæ ·æœ¬ID
        # ç¬¬ä¸€ä¸ªå•å…ƒæ ¼å†™ 'Taxon' æˆ– 'Feature'ï¼Œåé¢æ˜¯å…·ä½“çš„æ ·æœ¬ID
        writer.writerow(['Feature'] + sample_ids)

        # æŒ‰å­—æ¯é¡ºåºå†™å…¥æ¯ä¸ªç‰©ç§ï¼ˆFeatureï¼‰åŠå…¶ä¸°åº¦æ•°æ®
        # æ’åºå¯ä»¥ç¡®ä¿æ¯æ¬¡è¿è¡Œè¾“å‡ºçš„è¡Œé¡ºåºä¸€è‡´
        sorted_features = sorted(features_data.keys())

        for feature_name in sorted_features:
            abundances = features_data[feature_name]
            # ç¡®ä¿ä¸°åº¦åˆ—è¡¨é•¿åº¦ä¸æ ·æœ¬æ•°é‡åŒ¹é…
            if len(abundances) == len(sample_ids):
                # å°†ç‰©ç§åç§°å’Œå…¶å¯¹åº”çš„ä¸°åº¦åˆ—è¡¨å†™å…¥ä¸€è¡Œ
                writer.writerow([feature_name] + abundances)
            else:
                print(
                    f"âš ï¸ è­¦å‘Š: ç‰©ç§ '{feature_name}' çš„æ•°æ®ç‚¹æ•°é‡ ({len(abundances)}) ä¸æ ·æœ¬æ•°é‡ ({len(sample_ids)}) ä¸åŒ¹é…ï¼Œå·²è·³è¿‡ã€‚")

    print(f"ğŸ‰ è½¬æ¢å®Œæˆï¼è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜è‡³: {output_csv_path}")


if __name__ == '__main__':
    # --- è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æ ---
    parser = argparse.ArgumentParser(
        description="å°† LEfSe è¾“å…¥æ–‡ä»¶ (.in) è½¬æ¢ä¸ºäººç±»å¯è¯»çš„ CSV æ ¼å¼ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('-i', '--input', required=True, help="è¾“å…¥çš„ lefse.in æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument('-s', '--samples', required=True,
                        help="æ ·æœ¬ä¿¡æ¯æ–‡ä»¶è·¯å¾„ (Tabæˆ–ç©ºæ ¼åˆ†éš”, ç¬¬1åˆ—æ ·æœ¬ID, ç¬¬2åˆ—åˆ†ç»„ID)ã€‚")
    parser.add_argument('-o', '--output', required=True, help="è¾“å‡ºçš„ CSV æ–‡ä»¶è·¯å¾„ã€‚")

    args = parser.parse_args()

    # --- è°ƒç”¨ä¸»å‡½æ•° ---
    convert_lefse_to_csv(args.input, args.samples, args.output)